{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a2abfd-2f64-4882-b561-3267a2338a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../../../../../STUDY/CRAWLING'))\n",
    "from SeleniumScraper import SeleniumScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a270cc88-99cd-4ece-ad7b-e8d9e7aa5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f35f0e91-f284-4ab8-92a7-dadebf4ddb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_review_details(li_element):\n",
    "    \n",
    "    \"\"\" Extract Review Deatails \"\"\"\n",
    "    \n",
    "    review_dict = {}\n",
    "    try:\n",
    "        ############################# 각 요소 ############################# \n",
    "        date = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'timestamp_reviewDate__dsF9n'))).text\n",
    "        rating = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'review-rating_ratingLabel__0_Hk9'))).text\n",
    "        one_line_review = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'heading_Heading__BqX5J.heading_Level3__X81KK'))).text\n",
    "        job = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'review-avatar_avatarLabel__P15ey'))).text\n",
    "        history = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'text-with-icon_TextWithIcon__5ZZqT'))).text\n",
    "        \n",
    "        try:\n",
    "            region = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'review-avatar_tagsContainer__9NCNs'))).text.split('\\n')[1]\n",
    "        except:\n",
    "            region=None\n",
    "        \n",
    "        pros = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-text-PROS\"]'))).text\n",
    "        cons = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-text-CONS\"]'))).text\n",
    "    \n",
    "        try:\n",
    "            show_more_button_class_nm = 'expand-button_ExpandButton__Wevvg'\n",
    "            show_more_button = WebDriverWait(li_element, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, show_more_button_class_nm))\n",
    "            )\n",
    "            show_more_button.click()\n",
    "    \n",
    "            feedback = WebDriverWait(li_element, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-text-FEEDBACK\"]'))\n",
    "            ).text\n",
    "        except:\n",
    "            feedback = None\n",
    "        \n",
    "        review_dict.update({\n",
    "            'Date': date,\n",
    "            'Job': job,\n",
    "            'History': history,\n",
    "            'Region': region if region else None,\n",
    "            'One_line_review': one_line_review,\n",
    "            'Rating': rating,\n",
    "            'Pros': pros,\n",
    "            'Cons': cons,\n",
    "            'Feedback': feedback# if feedback else None,\n",
    "        })\n",
    "\n",
    "        ############################# Sub Rating ############################# \n",
    "        try:\n",
    "            rating_element = li_element.find_element(By.CLASS_NAME, 'review-details-bar_reviewRatingAndFeaturedContainer__J8iG9')\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # ActionChains로 마우스를 이동시켜서 숨겨진 요소 표시\n",
    "            ActionChains(driver).move_to_element(rating_element).perform()\n",
    "            tooltip = WebDriverWait(driver, 5).until(\n",
    "                EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-subratings-tooltip\"]'))\n",
    "            )\n",
    "            # 숨겨진 요소의 요소 전부 값 가져오기\n",
    "            tooltip_html = tooltip.get_attribute(\"innerHTML\")\n",
    "            soup = BeautifulSoup(tooltip_html, 'html.parser')\n",
    "            \n",
    "            # 각 항목을 포함하는 div를 찾음\n",
    "            sub_ratings = soup.find_all('div', class_='review-rating_subRating__0Q_Z0')\n",
    "            \n",
    "            # 각 항목별로 점수 계산\n",
    "            for rating in sub_ratings:\n",
    "                # 항목 이름 가져오기\n",
    "                category = rating.find('span', class_='review-rating_subRatingText__Wn3AL').text.strip()\n",
    "                # --outline-percentage가 0%인 별만 카운트\n",
    "                stars = len(rating.find_all('div', style=\"--outline-percentage: 0%;\"))# 별 개수를 점수로 계산\n",
    "                review_dict[category] = stars\n",
    "                \n",
    "        except Exception:\n",
    "            pass;\n",
    "\n",
    "        ############################# Check List #############################\n",
    "        try:\n",
    "            check_list = li_element.find_elements(By.CLASS_NAME, 'review-details_experienceContainer__2W06X')\n",
    "\n",
    "            # 각 요소의 전체 HTML 출력\n",
    "            for element in check_list:\n",
    "                html_content = element.get_attribute(\"outerHTML\")\n",
    "                soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "                # 클래스 마지막 부분에 따른 값 매핑 딕셔너리\n",
    "                class_value_map = {\n",
    "                    'LGHYG': 'V',\n",
    "                    'BCbQd': 'empty',\n",
    "                    'jmOo8': '-',\n",
    "                    '0GqA8': 'X'\n",
    "                }\n",
    "\n",
    "                # 각 span 요소의 텍스트와 해당 클래스명 끝자리에 따른 값 저장\n",
    "                for span in soup.find_all('span'):\n",
    "                    # span 텍스트 가져오기\n",
    "                    span_text = span.text.strip()\n",
    "                    # 부모 div 요소의 마지막 클래스명 부분 가져오기\n",
    "                    parent_div = span.find_parent('div')\n",
    "                    last_class_part = parent_div['class'][-1].split('_')[-1]\n",
    "                    # 매핑된 값을 result 딕셔너리에 추가\n",
    "                    review_dict[span_text] = class_value_map.get(last_class_part, 'Unknown')\n",
    "        \n",
    "        except Exception:\n",
    "            pass;\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting review details: {e}\")\n",
    "    \n",
    "    return review_dict, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c22ca2-e281-4835-9232-de2437f1bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 회사 목록\n",
    "with open('all_company_list.pickle', 'rb') as f:\n",
    "    company_list = pickle.load(f)\n",
    "# 완료된 회사 목록\n",
    "with open('complete_company_list.pickle', 'rb') as f:\n",
    "    complete_company_list = pickle.load(f)\n",
    "\n",
    "# 남은 회사 목록\n",
    "extra_companies = [company for company in company_list if company not in complete_company_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f5f267-1bb4-4f45-9650-4c21b80bf718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (130.0.6723.69) detected in PATH at /usr/local/bin/chromedriver might not be compatible with the detected chrome version (131.0.6778.140); currently, chromedriver 131.0.6778.108 is recommended for chrome 131.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing company: Lithia Motors, Inc.\n",
      "filterling button not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                  | 0/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 00:00:00 2021-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                  | 0/86 [00:25<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m li_elements \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviewsFeed\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/ol/li\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#?\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m li_element \u001b[38;5;129;01min\u001b[39;00m li_elements:\n\u001b[0;32m---> 69\u001b[0m     review_dict, date \u001b[38;5;241m=\u001b[39m \u001b[43mextract_review_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli_element\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# 2021.01.01 이전이면 종료\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mstrptime(date, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mextract_review_details\u001b[0;34m(li_element)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     rating_element \u001b[38;5;241m=\u001b[39m li_element\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview-details-bar_reviewRatingAndFeaturedContainer__J8iG9\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# ActionChains로 마우스를 이동시켜서 숨겨진 요소 표시\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     ActionChains(driver)\u001b[38;5;241m.\u001b[39mmove_to_element(rating_element)\u001b[38;5;241m.\u001b[39mperform()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 클래스 초기화\n",
    "scraper = SeleniumScraper(\n",
    "    search_url=\"https://www.glassdoor.com/Reviews/index.htm?overall_rating_low=3.5&page=1&locId=1147401&locType=C&occ=Data%20Scientist\"\n",
    ")\n",
    "\n",
    "review_df = pd.DataFrame()\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for company in extra_companies:\n",
    "    \n",
    "    print(f\"Processing company: {company}\")\n",
    "    \n",
    "    file_name = f\"reviews_{company}.csv\"\n",
    "    # 검색\n",
    "    search_object = '//*[@id=\"companyAutocomplete-companyDiscover-employerSearch\"]'  # 검색 입력창 XPath\n",
    "    button_object = '//*[@id=\"Explore\"]/div[2]/div/div/div[2]/button'  # 검색 버튼 XPath\n",
    "    scraper.get_search(company, search_object, button_object)\n",
    "    \n",
    "    # 검색 결과에서 첫 번째 링크 클릭\n",
    "    search_list_xpath = '//*[@id=\"Discover\"]/div/div/div[1]/div[1]/div[1]'\n",
    "    driver = scraper.get_driver() # driver 객체 가져오기\n",
    "    first_result = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, search_list_xpath))\n",
    "    )\n",
    "    first_result_link = first_result.find_elements(By.CSS_SELECTOR, 'a')[0].get_attribute('href')\n",
    "    driver.get(first_result_link)\n",
    "\n",
    "    # 리뷰 페이지로 이동\n",
    "    a_element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '#reviews > a')))\n",
    "    review_href = a_element.get_attribute('href')\n",
    "    driver.get(review_href)\n",
    "\n",
    "    # 날짜 기준 정렬\n",
    "    sort_date_parent_class_nm = 'sort_SortButton__hzneq'\n",
    "    sort_date_child_datatest = '[data-test=\"select-dropdown-value-DATE\"]'\n",
    "    scraper.filterlng(sort_date_parent_class_nm, sort_date_child_datatest)\n",
    "\n",
    "    try:\n",
    "        # 기존 filterling clear\n",
    "        clear_parent_class_nm = 'filter-menu_FilterMenuButton__NOhT8'\n",
    "        clear_child_datatest = '[data-test=\"filter-menu-clear-button\"]'\n",
    "        scraper.filterlng(clear_parent_class_nm, clear_child_datatest)\n",
    "    except:\n",
    "        with open('filtering_fail_list.pickle', 'rb') as f: # Read\n",
    "            filtering_fail_list = pickle.load(f)\n",
    "            filtering_fail_list.append(company) # Append\n",
    "            \n",
    "        with open(\"filtering_fail_list.pickle\",\"wb\") as f: # Write\n",
    "            pickle.dump(filtering_fail_list, f)\n",
    "        print('filterling button not found')\n",
    "    \n",
    "    # last page search\n",
    "    last_p_elements = driver.find_elements(By.CSS_SELECTOR, \"p.pagination_PageNumberText__zy_hr\")\n",
    "    last_page = int([p.text for p in last_p_elements][-1])\n",
    "    \n",
    "    # pagination\n",
    "    iteration_count = 0\n",
    "    for page in tqdm(range(1, last_page + 1)):\n",
    "        iteration_count+=1\n",
    "        if page > 1:\n",
    "            review_url = review_href.replace('.htm', f'_P{page}.htm')\n",
    "            driver.get(review_url)\n",
    "        else:\n",
    "            pass;\n",
    "\n",
    "        try:\n",
    "            li_elements = driver.find_elements(By.XPATH, '//*[@id=\"ReviewsFeed\"]/ol/li') #?\n",
    "            for li_element in li_elements:\n",
    "                review_dict, date = extract_review_details(li_element)\n",
    "                \n",
    "                # 2021.01.01 이전이면 종료\n",
    "                date = datetime.datetime.strptime(date, \"%b %d, %Y\")\n",
    "                target_date = datetime.datetime(2021, 1, 1)\n",
    "                print(date, target_date)\n",
    "                if date < target_date:\n",
    "                    break;\n",
    "                    \n",
    "                review_dict['Company'] = company\n",
    "                review_df = pd.concat([review_df, pd.DataFrame([review_dict])], ignore_index=True)\n",
    "\n",
    "            if iteration_count%100:\n",
    "                review_df.to_csv(f'tmp_company_review/{file_name}', index=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing reviews on page {page} for {company}: {e}\")\n",
    "    \n",
    "    result_df = pd.concat([result_df, review_df], ignore_index=True)\n",
    "    result_df.to_csv('company_review_data/complete_'+file_name, index=False)\n",
    "    \n",
    "    with open('complete_company_list.pickle', 'rb') as f:\n",
    "        complete_company_list = pickle.load(f)\n",
    "    complete_company_list.append(company)\n",
    "    \n",
    "    with open(\"complete_company_list.pickle\",\"wb\") as f:\n",
    "        pickle.dump(complete_company_list, f)\n",
    "    \n",
    "    print(f\"{company} complete !\")\n",
    "\n",
    "print(\"Scraping complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8232470-81f6-4686-99c3-b3b5d2516e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2a08d-ed3b-4456-8d2c-1f420914bd63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
