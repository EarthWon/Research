{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d764014d-a58e-496b-94db-2bff977830f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오브젝트 나열하는 중: 11, 완료.\n",
      "오브젝트 개수 세는 중: 100% (11/11), 완료.\n",
      "Delta compression using up to 8 threads\n",
      "오브젝트 압축하는 중: 100% (6/6), 완료.\n",
      "오브젝트 쓰는 중: 100% (6/6), 770 bytes | 770.00 KiB/s, 완료.\n",
      "Total 6 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/EarthWon/Research.git\n",
      "   cb78ed6..09e5b23  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a2abfd-2f64-4882-b561-3267a2338a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../../../../../STUDY/CRAWLING'))\n",
    "# sys.path.append(os.path.abspath('../../../Selenium/'))\n",
    "from SeleniumScraper import SeleniumScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a270cc88-99cd-4ece-ad7b-e8d9e7aa5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35f0e91-f284-4ab8-92a7-dadebf4ddb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_review_details(li_element):\n",
    "    \n",
    "    \"\"\" Extract Review Deatails \"\"\"\n",
    "    \n",
    "    review_dict = {}\n",
    "    try:\n",
    "        ############################# 각 요소 ############################# \n",
    "        date = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'timestamp_reviewDate__dsF9n'))).text\n",
    "        rating = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'review-rating_ratingLabel__0_Hk9'))).text\n",
    "        one_line_review = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'heading_Heading__BqX5J.heading_Level3__X81KK'))).text\n",
    "        job = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'review-avatar_avatarLabel__P15ey'))).text\n",
    "        history = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'text-with-icon_TextWithIcon__5ZZqT'))).text\n",
    "        \n",
    "        # 2021.01.01 이전이면 종료\n",
    "        date = datetime.datetime.strptime(date, \"%b %d, %Y\")\n",
    "        target_date = datetime.datetime(2021, 1, 1)\n",
    "        if date < target_date:\n",
    "            print('Before 21.01.01')\n",
    "            return 0,0\n",
    "        \n",
    "        try:\n",
    "            region = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'review-avatar_tagsContainer__9NCNs'))).text.split('\\n')[1]\n",
    "        except:\n",
    "            region=None\n",
    "        \n",
    "        pros = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-text-PROS\"]'))).text\n",
    "        cons = WebDriverWait(li_element, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-text-CONS\"]'))).text\n",
    "    \n",
    "        try:\n",
    "            show_more_button_class_nm = 'expand-button_ExpandButton__Wevvg'\n",
    "            show_more_button = WebDriverWait(li_element, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, show_more_button_class_nm))\n",
    "            )\n",
    "            show_more_button.click()\n",
    "    \n",
    "            feedback = WebDriverWait(li_element, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-text-FEEDBACK\"]'))\n",
    "            ).text\n",
    "        except:\n",
    "            feedback = None\n",
    "        \n",
    "        review_dict.update({\n",
    "            'Date': date,\n",
    "            'Job': job,\n",
    "            'History': history,\n",
    "            'Region': region if region else None,\n",
    "            'One_line_review': one_line_review,\n",
    "            'Rating': rating,\n",
    "            'Pros': pros,\n",
    "            'Cons': cons,\n",
    "            'Feedback': feedback# if feedback else None,\n",
    "        })\n",
    "\n",
    "        ############################# Sub Rating ############################# \n",
    "        try:\n",
    "            rating_element = li_element.find_element(By.CLASS_NAME, 'review-details-bar_reviewRatingAndFeaturedContainer__J8iG9')\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # ActionChains로 마우스를 이동시켜서 숨겨진 요소 표시\n",
    "            ActionChains(driver).move_to_element(rating_element).perform()\n",
    "            tooltip = WebDriverWait(driver, 5).until(\n",
    "                EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-test=\"review-subratings-tooltip\"]'))\n",
    "            )\n",
    "            # 숨겨진 요소의 요소 전부 값 가져오기\n",
    "            tooltip_html = tooltip.get_attribute(\"innerHTML\")\n",
    "            soup = BeautifulSoup(tooltip_html, 'html.parser')\n",
    "            \n",
    "            # 각 항목을 포함하는 div를 찾음\n",
    "            sub_ratings = soup.find_all('div', class_='review-rating_subRating__0Q_Z0')\n",
    "            \n",
    "            # 각 항목별로 점수 계산\n",
    "            for rating in sub_ratings:\n",
    "                # 항목 이름 가져오기\n",
    "                category = rating.find('span', class_='review-rating_subRatingText__Wn3AL').text.strip()\n",
    "                # --outline-percentage가 0%인 별만 카운트\n",
    "                stars = len(rating.find_all('div', style=\"--outline-percentage: 0%;\"))# 별 개수를 점수로 계산\n",
    "                review_dict[category] = stars\n",
    "                \n",
    "        except Exception:\n",
    "            pass;\n",
    "\n",
    "        ############################# Check List #############################\n",
    "        try:\n",
    "            check_list = li_element.find_elements(By.CLASS_NAME, 'review-details_experienceContainer__2W06X')\n",
    "\n",
    "            # 각 요소의 전체 HTML 출력\n",
    "            for element in check_list:\n",
    "                html_content = element.get_attribute(\"outerHTML\")\n",
    "                soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "                # 클래스 마지막 부분에 따른 값 매핑 딕셔너리\n",
    "                class_value_map = {\n",
    "                    'LGHYG': 'V',\n",
    "                    'BCbQd': 'empty',\n",
    "                    'jmOo8': '-',\n",
    "                    '0GqA8': 'X'\n",
    "                }\n",
    "\n",
    "                # 각 span 요소의 텍스트와 해당 클래스명 끝자리에 따른 값 저장\n",
    "                for span in soup.find_all('span'):\n",
    "                    # span 텍스트 가져오기\n",
    "                    span_text = span.text.strip()\n",
    "                    # 부모 div 요소의 마지막 클래스명 부분 가져오기\n",
    "                    parent_div = span.find_parent('div')\n",
    "                    last_class_part = parent_div['class'][-1].split('_')[-1]\n",
    "                    # 매핑된 값을 result 딕셔너리에 추가\n",
    "                    review_dict[span_text] = class_value_map.get(last_class_part, 'Unknown')\n",
    "        \n",
    "        except Exception:\n",
    "            pass;\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting review details: {e}\")\n",
    "    \n",
    "    return review_dict, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c22ca2-e281-4835-9232-de2437f1bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 회사 목록\n",
    "with open('all_company_list.pickle', 'rb') as f:\n",
    "    company_list = pickle.load(f)\n",
    "# 완료된 회사 목록\n",
    "with open('complete_company_list.pickle', 'rb') as f:\n",
    "    complete_company_list = pickle.load(f)\n",
    "\n",
    "# 남은 회사 목록\n",
    "extra_companies = [company for company in company_list if company not in complete_company_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b110e130-5d9b-4b80-a721-f94f62578886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['International Business Machines Corp.',\n",
       " 'Cognizant Technology Solutions Corp.',\n",
       " 'Microsoft Corp.',\n",
       " 'Oracle Corp.',\n",
       " 'The Home Depot, Inc.',\n",
       " \"Lowe's Companies, Inc.\",\n",
       " 'Cisco Systems, Inc.',\n",
       " 'DXC Technology Co.',\n",
       " 'Automatic Data Processing, Inc.',\n",
       " 'PepsiCo, Inc.',\n",
       " 'Salesforce, Inc.',\n",
       " 'UnitedHealth Group, Inc.',\n",
       " 'Robert Half, Inc.',\n",
       " 'Thermo Fisher Scientific, Inc.',\n",
       " 'Jones Lang LaSalle, Inc.',\n",
       " 'Amdocs Ltd.',\n",
       " 'Adobe, Inc.',\n",
       " 'Pfizer Inc.',\n",
       " 'QUALCOMM, Inc.',\n",
       " 'HP, Inc.',\n",
       " 'CarMax, Inc.',\n",
       " 'Jacobs Engineering Group, Inc.',\n",
       " 'WSP Global, Inc.',\n",
       " 'Flex Ltd.',\n",
       " 'The Cigna Group',\n",
       " 'AutoZone, Inc.',\n",
       " 'Ecolab, Inc.',\n",
       " 'NVIDIA Corp.',\n",
       " 'TELUS International (CDA), Inc.',\n",
       " 'McKesson Corp.',\n",
       " 'Elevance Health, Inc.',\n",
       " 'Electronic Arts, Inc.',\n",
       " 'L3Harris Technologies, Inc.',\n",
       " 'Motorola Solutions, Inc.',\n",
       " 'Advance Auto Parts, Inc.',\n",
       " 'Advanced Micro Devices, Inc.',\n",
       " 'Applied Materials, Inc.',\n",
       " 'Cardinal Health, Inc.',\n",
       " 'CDW Corp.',\n",
       " 'Western Digital Corp.',\n",
       " 'General Mills, Inc.',\n",
       " 'W.W. Grainger, Inc.',\n",
       " 'Avis Budget Group, Inc.',\n",
       " 'J.B. Hunt Transport Services, Inc.',\n",
       " 'International Bank for Reconstruction & Development',\n",
       " 'Jabil, Inc.',\n",
       " 'Synopsys, Inc.',\n",
       " \"O'Reilly Automotive, Inc.\",\n",
       " 'The Williams Cos., Inc.',\n",
       " 'Williams-Sonoma, Inc.',\n",
       " 'Lear Corp.',\n",
       " 'Colliers International Group, Inc.',\n",
       " \"Carter's, Inc.\",\n",
       " 'The Western Union Co.',\n",
       " 'GLOBALFOUNDRIES, Inc.',\n",
       " 'Cadence Design Systems, Inc.',\n",
       " 'Illumina, Inc.',\n",
       " 'Seagate Technology Holdings Plc',\n",
       " 'Iron Mountain, Inc.',\n",
       " 'Keysight Technologies, Inc.',\n",
       " 'Principal Financial Group, Inc.',\n",
       " 'Stantec, Inc.',\n",
       " 'Ryder System, Inc.',\n",
       " 'Arrow Electronics, Inc.',\n",
       " 'National Instruments Corp.',\n",
       " 'Agilent Technologies, Inc.',\n",
       " 'ManpowerGroup, Inc.',\n",
       " 'Massachusetts Mutual Life Insurance Co.',\n",
       " 'Palo Alto Networks, Inc.',\n",
       " 'Akamai Technologies, Inc.',\n",
       " 'Enbridge, Inc.',\n",
       " 'Zebra Technologies Corp.',\n",
       " 'The Hershey Co.',\n",
       " 'Charles River Laboratories International, Inc.',\n",
       " 'Cencora, Inc.',\n",
       " 'Air Products & Chemicals, Inc.',\n",
       " 'Canadian National Railway Co.',\n",
       " 'Henry Schein, Inc.',\n",
       " 'AutoNation, Inc.',\n",
       " 'BorgWarner, Inc.',\n",
       " 'Avnet, Inc.',\n",
       " 'Trimble, Inc.',\n",
       " 'Lam Research Corp.',\n",
       " 'Regeneron Pharmaceuticals, Inc.',\n",
       " 'Public Storage',\n",
       " 'AvalonBay Communities, Inc.',\n",
       " 'Dentsply Sirona, Inc.',\n",
       " 'Crown Castle, Inc.',\n",
       " 'Waters Corp.',\n",
       " 'Bio-Rad Laboratories, Inc.',\n",
       " 'American Tower Corp.',\n",
       " 'Ball Corp.',\n",
       " 'Xylem, Inc.',\n",
       " 'Avantor, Inc.',\n",
       " 'TD SYNNEX Corp.',\n",
       " 'Celestica, Inc.',\n",
       " 'Ontario Power Generation, Inc.',\n",
       " 'Kinder Morgan, Inc.',\n",
       " 'Polaris Inc.',\n",
       " 'Sun Life Financial, Inc.',\n",
       " 'LKQ Corp.',\n",
       " 'Sensata Technologies Holding Plc',\n",
       " 'Hasbro, Inc.',\n",
       " 'Nasdaq, Inc.',\n",
       " 'Equity Residential',\n",
       " 'Guidewire Software, Inc.',\n",
       " 'Mohawk Industries, Inc.',\n",
       " 'Mettler-Toledo International, Inc.',\n",
       " 'Zoetis, Inc.',\n",
       " 'Teck Resources Limited',\n",
       " 'Ingersoll Rand, Inc.',\n",
       " 'The New York Times Co.',\n",
       " 'News Corp.',\n",
       " 'Valvoline, Inc.',\n",
       " 'Opendoor Technologies, Inc.',\n",
       " 'Inter-American Development Bank',\n",
       " 'Lithia Motors, Inc.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_companies = extra_companies.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f5f267-1bb4-4f45-9650-4c21b80bf718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (130.0.6723.69) detected in PATH at /usr/local/bin/chromedriver might not be compatible with the detected chrome version (131.0.6778.140); currently, chromedriver 131.0.6778.108 is recommended for chrome 131.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing company: Lithia Motors, Inc.\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x00000001080bbf68 chromedriver + 7110504\n1   chromedriver                        0x00000001080b3f6a chromedriver + 7077738\n2   chromedriver                        0x0000000107a550f0 chromedriver + 397552\n3   chromedriver                        0x0000000107aa1383 chromedriver + 709507\n4   chromedriver                        0x0000000107aa1681 chromedriver + 710273\n5   chromedriver                        0x0000000107ae6e14 chromedriver + 994836\n6   chromedriver                        0x0000000107ac593d chromedriver + 858429\n7   chromedriver                        0x0000000107ae4234 chromedriver + 983604\n8   chromedriver                        0x0000000107ac56b3 chromedriver + 857779\n9   chromedriver                        0x0000000107a94182 chromedriver + 655746\n10  chromedriver                        0x0000000107a9515e chromedriver + 659806\n11  chromedriver                        0x00000001080813b0 chromedriver + 6869936\n12  chromedriver                        0x00000001080852e4 chromedriver + 6886116\n13  chromedriver                        0x00000001080639b7 chromedriver + 6748599\n14  chromedriver                        0x0000000108085d6e chromedriver + 6888814\n15  chromedriver                        0x0000000108052c84 chromedriver + 6679684\n16  chromedriver                        0x00000001080a2838 chromedriver + 7006264\n17  chromedriver                        0x00000001080a29f6 chromedriver + 7006710\n18  chromedriver                        0x00000001080b3b78 chromedriver + 7076728\n19  libsystem_pthread.dylib             0x00007ff81368018b _pthread_start + 99\n20  libsystem_pthread.dylib             0x00007ff81367bae3 thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m search_list_xpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiscover\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/div/div/div[1]/div[1]/div[1]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m driver \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mget_driver() \u001b[38;5;66;03m# driver 객체 가져오기\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m first_result \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_list_xpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m first_result_link \u001b[38;5;241m=\u001b[39m first_result\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(first_result_link)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n0   chromedriver                        0x00000001080bbf68 chromedriver + 7110504\n1   chromedriver                        0x00000001080b3f6a chromedriver + 7077738\n2   chromedriver                        0x0000000107a550f0 chromedriver + 397552\n3   chromedriver                        0x0000000107aa1383 chromedriver + 709507\n4   chromedriver                        0x0000000107aa1681 chromedriver + 710273\n5   chromedriver                        0x0000000107ae6e14 chromedriver + 994836\n6   chromedriver                        0x0000000107ac593d chromedriver + 858429\n7   chromedriver                        0x0000000107ae4234 chromedriver + 983604\n8   chromedriver                        0x0000000107ac56b3 chromedriver + 857779\n9   chromedriver                        0x0000000107a94182 chromedriver + 655746\n10  chromedriver                        0x0000000107a9515e chromedriver + 659806\n11  chromedriver                        0x00000001080813b0 chromedriver + 6869936\n12  chromedriver                        0x00000001080852e4 chromedriver + 6886116\n13  chromedriver                        0x00000001080639b7 chromedriver + 6748599\n14  chromedriver                        0x0000000108085d6e chromedriver + 6888814\n15  chromedriver                        0x0000000108052c84 chromedriver + 6679684\n16  chromedriver                        0x00000001080a2838 chromedriver + 7006264\n17  chromedriver                        0x00000001080a29f6 chromedriver + 7006710\n18  chromedriver                        0x00000001080b3b78 chromedriver + 7076728\n19  libsystem_pthread.dylib             0x00007ff81368018b _pthread_start + 99\n20  libsystem_pthread.dylib             0x00007ff81367bae3 thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "# 클래스 초기화\n",
    "scraper = SeleniumScraper(\n",
    "    search_url=\"https://www.glassdoor.com/Reviews/index.htm?overall_rating_low=3.5&page=1&locId=1147401&locType=C&occ=Data%20Scientist\"\n",
    ")\n",
    "\n",
    "review_df = pd.DataFrame()\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for company in extra_companies:\n",
    "    \n",
    "    print(f\"Processing company: {company}\")\n",
    "    \n",
    "    file_name = f\"reviews_{company}.csv\"\n",
    "    # 검색\n",
    "    search_object = '//*[@id=\"companyAutocomplete-companyDiscover-employerSearch\"]'  # 검색 입력창 XPath\n",
    "    button_object = '//*[@id=\"Explore\"]/div[2]/div/div/div[2]/button'  # 검색 버튼 XPath\n",
    "    scraper.get_search(company, search_object, button_object)\n",
    "    \n",
    "    # 검색 결과에서 첫 번째 링크 클릭\n",
    "    search_list_xpath = '//*[@id=\"Discover\"]/div/div/div[1]/div[1]/div[1]'\n",
    "    driver = scraper.get_driver() # driver 객체 가져오기\n",
    "    first_result = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, search_list_xpath))\n",
    "    )\n",
    "    first_result_link = first_result.find_elements(By.CSS_SELECTOR, 'a')[0].get_attribute('href')\n",
    "    driver.get(first_result_link)\n",
    "\n",
    "    # 리뷰 페이지로 이동\n",
    "    a_element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '#reviews > a')))\n",
    "    review_href = a_element.get_attribute('href')\n",
    "    driver.get(review_href)\n",
    "\n",
    "    # 날짜 기준 정렬\n",
    "    sort_date_parent_class_nm = 'sort_SortButton__hzneq'\n",
    "    sort_date_child_datatest = '[data-test=\"select-dropdown-value-DATE\"]'\n",
    "    scraper.filterlng(sort_date_parent_class_nm, sort_date_child_datatest)\n",
    "\n",
    "    try:\n",
    "        # 기존 filterling clear\n",
    "        clear_parent_class_nm = 'filter-menu_FilterMenuButton__NOhT8'\n",
    "        clear_child_datatest = '[data-test=\"filter-menu-clear-button\"]'\n",
    "        scraper.filterlng(clear_parent_class_nm, clear_child_datatest)\n",
    "    except:\n",
    "        with open('filtering_fail_list.pickle', 'rb') as f: # Read\n",
    "            filtering_fail_list = pickle.load(f)\n",
    "            filtering_fail_list.append(company) # Append\n",
    "            \n",
    "        with open(\"filtering_fail_list.pickle\",\"wb\") as f: # Write\n",
    "            pickle.dump(filtering_fail_list, f)\n",
    "        print('filterling button not found')\n",
    "    \n",
    "    # last page search\n",
    "    last_p_elements = driver.find_elements(By.CSS_SELECTOR, \"p.pagination_PageNumberText__zy_hr\")\n",
    "    last_page = int([p.text for p in last_p_elements][-1])\n",
    "    \n",
    "    # pagination\n",
    "    iteration_count = 0\n",
    "    for page in tqdm(range(1, last_page + 1)):\n",
    "        iteration_count+=1\n",
    "        if page > 1:\n",
    "            review_url = review_href.replace('.htm', f'_P{page}.htm')\n",
    "            driver.get(review_url)\n",
    "        else:\n",
    "            pass;\n",
    "\n",
    "        try:\n",
    "            li_elements = driver.find_elements(By.XPATH, '//*[@id=\"ReviewsFeed\"]/ol/li') #?\n",
    "            for li_element in li_elements:\n",
    "                review_dict, date = extract_review_details(li_element)\n",
    "                if date==0:\n",
    "                    break;\n",
    "                review_dict['Company'] = company\n",
    "                review_df = pd.concat([review_df, pd.DataFrame([review_dict])], ignore_index=True)\n",
    "\n",
    "            if iteration_count%100:\n",
    "                review_df.to_csv(f'tmp_company_review/{file_name}', index=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing reviews on page {page} for {company}: {e}\")\n",
    "    \n",
    "    result_df = pd.concat([result_df, review_df], ignore_index=True)\n",
    "    result_df.to_csv('company_review_data/complete_'+file_name, index=False)\n",
    "    \n",
    "    with open('complete_company_list.pickle', 'rb') as f:\n",
    "        complete_company_list = pickle.load(f)\n",
    "    complete_company_list.append(company)\n",
    "    \n",
    "    with open(\"complete_company_list.pickle\",\"wb\") as f:\n",
    "        pickle.dump(complete_company_list, f)\n",
    "    \n",
    "    print(f\"{company} complete !\")\n",
    "\n",
    "print(\"Scraping complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8232470-81f6-4686-99c3-b3b5d2516e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2a08d-ed3b-4456-8d2c-1f420914bd63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
